{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty Quantification Pipeline - Version 1\n",
    "This notebook contains the complete use of the version 1 of the Uncertainity Quantification Pipeilne for EXPERT 2.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Uncertainity Quantification(UQ) Pipeline aims to measure the uncertainity of decoder/autoregressive styled large language model through various entropy and confidence scores. \n",
    "\n",
    "In this version of our UQ Pipeline, we experiment with various Unsupervised Methods for Uncertainity Estimation. Unsupervised Methods for Uncertainity Estimation are the ones which do not involve any type of caliberation or training. The models used in this version are frozen, which means that their weights are not updated in any way.\n",
    "\n",
    "In this version of our UQ Pipeline, we present the following 4 different types of uncertainty estimation algorithms:\n",
    "\n",
    "1. Entropy\n",
    "2. Normalized Entropy\n",
    "3. Lexical Similarity\n",
    "4. Semantic Entropy\n",
    "\n",
    "These 4 entropy scores represent the extent of uncertainity shown by a given model for a given input prompt (question).\n",
    "These are among the 4 widely used uncertainity measures in the community, with Semantic Entropy being the state-of-the-art for Unsupervised Uncertainity Estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "The UQ Pipeline consists of 3 seperate classes:\n",
    "\n",
    "1. [Generation](#1-generation): This consists of all the different functions required to generate the output(s) for a given prompt(question)\n",
    "2. [Entropy](#2-entropy): This consists of all the different functions required to extract the entropy scores for a given set of generations\n",
    "3. [Pipeline](#3-pipeline): This combines both, Generation as well as Entropy classes for easy save and run for a given set of model and prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generation\n",
    "\n",
    "In this section we explore the Generation class of the UQ Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hora620/.conda/envs/expert_uqg/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the Generation Class\n",
    "from uq_pipeline import UQ_Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Using a GPT2-XL Model\n",
    "gen_pipeline = UQ_Generation(\"gpt2-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up the input\n",
    "question = \"Which knowledge of the aqueous solubility of TBP during Purex process conditions is important?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation Methods<sup>[[1]](#acknowledgments)</sup>, also known as Decoding Strategies, are how language models choose what words they should output once they know the probabilities.\n",
    "\n",
    "In this version we experiment with 3 different types of generation methods:\n",
    "1. Sampling with temperature: This method randomly picks the next token from a set of high-probablity tokens\n",
    "2. Nucleus (Top-p) Sampling: This method chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability ```p```\n",
    "3. Beam Search: This method keeps the most likely ```num_beams``` of hypotheses at each time step and eventually chooses the hypothesis that has the overall highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generating output for the given prompt(question). \n",
    "\n",
    "# Using the Beam Search Method we generate 10 seperate sequences(answers), \n",
    "# for the given input prompt(question), with 15 tokens generated for each sequence(answer)\n",
    "\n",
    "# Please note that the number of tokens more than 15 is not tested.\n",
    "outputs = gen_pipeline.gen_beam(question, num_tokens=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Getting probablities for each generated  \n",
    "gen_probs = gen_pipeline.get_probab(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nTBP is insoluble in water, but it is soluble in',\n",
       " '\\n\\nTBP is insoluble in water at room temperature. However,',\n",
       " '\\n\\nTBP is insoluble in water at room temperature. It is',\n",
       " '\\n\\nTBP is insoluble in water at room temperature. The sol',\n",
       " '\\n\\nTBP is insoluble in water at pH 7.4 and',\n",
       " '\\n\\nTBP is insoluble in water at pH 7.4.',\n",
       " '\\n\\nTBP is insoluble in water at room temperature, but soluble',\n",
       " '\\n\\nTBP is insoluble in water at pH 7.4,',\n",
       " '\\n\\nTBP is insoluble in water, but soluble in ethanol,',\n",
       " '\\n\\nTBP is insoluble in water at pH 7.0 and']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoding generated sequences into text\n",
    "gen_text = gen_pipeline.get_gen_text()\n",
    "gen_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Measuring Uncertainty\n",
    "\n",
    "In this section we explore various uncertainty estimation methods proposed for autoregressive styled generative LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Importing Entropy Class\n",
    "from uq_pipeline import UQ_Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Using generation probablities from Generation Class \n",
    "entropy_pipeline = UQ_Entropy(gen_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Predictive Entropy<sup>[[2]](#acknowledgments)</sup>\n",
    "\n",
    "The entropy is a statistical parameter which measures, in a certain sense, how much information is produced on the average for each letter of a text in the language. \n",
    "\n",
    "For a generated sequence, predictive entropy is the sum of product of conditional probablities of all tokens in S and their corresponding log values\n",
    "\n",
    "![image](img/pred_entropy.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the final predictive entropy for a given model, we average the predictive entropy for a set S of generated sequences for a given prompt x\n",
    "\n",
    "![image](img/pred_entropy_final.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Predictive Entropy\n",
    "\n",
    "entropy = entropy_pipeline.get_entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Normalized Predictive Entropy<sup>[[3]](#acknowledgments)</sup>\n",
    "\n",
    "It is similar to the predictive entropy, however, we normalize the sequence entropy by dividing it by the total number of tokens generated (N).\n",
    "\n",
    "![image](img/norm_entropy.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Normalized Entropy\n",
    "\n",
    "norm_entropy = entropy_pipeline.normalized_entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Lexical Similarity<sup>[[4]](#acknowledgments)</sup>\n",
    "\n",
    "Lexical similarity uses the average similarity of the answers in the answer\n",
    "set S\n",
    "\n",
    "![image](img/lex_sim.jpeg)\n",
    "\n",
    "where the sim if the Rouge-L score, and\n",
    "\n",
    "![image](img/lex_sim_C.jpeg)\n",
    "\n",
    "We invert the final Lexical Similarity Score to estimate the entropy in the generated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Lexical Similarity\n",
    "\n",
    "gen_sequences = gen_pipeline.gen_sequences\n",
    "gen_tokenizer = gen_pipeline.tokenizer\n",
    "\n",
    "lex_sim = entropy_pipeline.lexical_similarity(gen_sequences, gen_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Semantic Entropy<sup>[[5]](#acknowledgments)</sup>\n",
    "\n",
    "Semantic Entropy is a measure to estimate entropy for open-ended generations. In this method, we create multiple ```meaning sets, C```, which consists of various generated sequences(answers) for the same prompt(question) which are semantically similar. \n",
    "\n",
    "We then use the sum of the various ```meaning sets, C```, to calculate the final Semantic Entropy similar to the way we calcualte predictive entropy for a sequence.\n",
    "\n",
    "![image](img/sem_entropy.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 4. Semantic Entropy\n",
    "\n",
    "sem_uncertainty = entropy_pipeline.semantic_uncertainty(question, gen_sequences, gen_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 0.12755484166352649\n",
      "Normalized Entropy: 0.1622507373491923\n",
      "Lexical Similarity: 0.04075235109717865\n",
      "Semantic Entropy: 0.13229776088533252\n"
     ]
    }
   ],
   "source": [
    "print(f\"Entropy: {entropy}\")\n",
    "print(f\"Normalized Entropy: {norm_entropy}\")\n",
    "print(f\"Lexical Similarity: {lex_sim}\")\n",
    "print(f\"Semantic Entropy: {sem_uncertainty}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pipeline\n",
    "\n",
    "In this section we explore the Pipeline class of the UQ Pipeline. The Pipeline class combines the Generation and Entropy classes to provide a one-line way to generate text for a given prompt, as well as calcualte the various entropy values for the provided model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Import the Pipeline Class\n",
    "from uq_pipeline import UQ_Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Initial Pipeline\n",
    "uq_pipeline = UQ_Pipeline(prompt=question, model_name=\"gpt2-xl\",\n",
    "                          gen_method=\"sampling\", outpath='./output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Done.....\n",
      "\tBasic Entropy Done.....\n",
      "\tNormalized Entropy Done.....\n",
      "\tLexical Similarity Done.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSemantic Uncertainity Done.....\n",
      "Entropy Done.....\n",
      "Output JSON saved at:\n",
      "./output/23_03_2023_18_29_20.json\n"
     ]
    }
   ],
   "source": [
    "# Save output as JSON\n",
    "out_json = uq_pipeline.save_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the widget is loaded and the inputs (Generator Algorithm, Uncertainty Estimator and the Input Question) are selected, the widget displays series of answers to the input question with colored background corresponding to the rank in which the token is displayed. \n",
    "\n",
    "Hover on each token to view more related tokens with their corresponding probabilities. The widget also displays the uncertainty estimation measured in terms of Entropy, Normalized Entropy, Lexical Similarity and Semantic Uncertainty. \n",
    "\n",
    "Note: You can also use the input files provided as supplementary materials to test the Jupyter widget functionality. Please direct the ``data_path`` to the respective folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(User Message: If running this widget on a virtual machine, port forward 38327 to your local machine and then run the widget)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:38327/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f40b0c1ab80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<uq_widget.uqWidget.LoadWidget at 0x7f3eee906460>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging, sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "from uq_widget import uqWidget\n",
    "uqWidget.LoadWidget(data_path = './output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example demos\n",
    "![image](img/uq_widget_overall.jpeg)\n",
    "![image](img/uq_widget_hover.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] How to generate text: using different decoding methods for language generation with Transformers [[Link](https://huggingface.co/blog/how-to-generate)]\n",
    "\n",
    "[2] Shannon, C. E. (2001). A mathematical theory of communication. ACM SIGMOBILE mobile computing and communications review, 5(1), 3-55. [[Link](https://dl.acm.org/doi/abs/10.1145/584091.584093)]\n",
    "\n",
    "[3] Malinin, A., & Gales, M. (2020). Uncertainty estimation in autoregressive structured prediction. arXiv preprint arXiv:2002.07650. [[Link](https://arxiv.org/abs/2002.07650)]\n",
    "\n",
    "[4] Fomicheva, M., Sun, S., Yankovskaya, L., Blain, F., Guzm√°n, F., Fishel, M., ... & Specia, L. (2020). Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics, 8, 539-555. [[Link](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00330/96475/Unsupervised-Quality-Estimation-for-Neural-Machine)]\n",
    "\n",
    "[5] Kuhn, L., Gal, Y., & Farquhar, S. (2023). Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation. arXiv preprint arXiv:2302.09664. [[Link](https://arxiv.org/abs/2302.09664)] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-expert_uqg] *",
   "language": "python",
   "name": "conda-env-.conda-expert_uqg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2cc57cde21072730e8cc473580cfa6919a141d097b500b45e60fce58909adf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

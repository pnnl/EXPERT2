{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/people/muni616/anaconda3/envs/exp-brain-copy/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from typing import List, Tuple, Callable, Any, Dict, Optional\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Modern machine learning involves the use of sophisticated algorithms and computational models to enable computers to learn from data and make predictions or decisions without being explicitly programmed.\",\n",
    "    \"Deep learning, a subset of machine learning, has gained prominence in recent years, utilizing neural networks with multiple layers to process complex data and extract meaningful patterns.\",\n",
    "    \"One of the key advancements in modern machine learning is the availability of large amounts of data, which enables more accurate and robust models.\",\n",
    "    \"The use of GPUs (Graphics Processing Units) has revolutionized machine learning by significantly accelerating the training and inference processes, especially for deep learning algorithms.\",\n",
    "    \"Transfer learning has emerged as a powerful technique, allowing models to leverage knowledge gained from pre-training on large datasets and apply it to specific tasks with limited labeled data.\",\n",
    "    \"Reinforcement learning, an area of machine learning, focuses on training agents to make sequential decisions by interacting with an environment and receiving feedback in the form of rewards.\",\n",
    "    \"Explainable AI (Artificial Intelligence) is becoming increasingly important in modern machine learning, as it aims to provide understandable and interpretable explanations for the decisions made by AI systems.\",\n",
    "    \"AutoML (Automated Machine Learning) has gained popularity, aiming to automate the process of selecting and optimizing machine learning models, making it more accessible to non-experts.\",\n",
    "    \"Federated learning has emerged as a privacy-preserving approach in machine learning, enabling multiple devices or parties to collaboratively train models without sharing their raw data.\",\n",
    "    \"Modern machine learning techniques have found applications in various domains, including image and speech recognition, natural language processing, autonomous vehicles, healthcare, finance, and many others.\",\n",
    "    \"Machine learning algorithms can be broadly categorized into supervised learning, unsupervised learning, and reinforcement learning, each with its unique characteristics and applications.\",\n",
    "    \"Supervised learning involves training a model on labeled data, where the desired output is known, allowing the model to learn patterns and make predictions on new, unseen data.\",\n",
    "    \"Unsupervised learning focuses on discovering patterns and structures in unlabeled data, without specific target outputs, and is often used for tasks like clustering and dimensionality reduction.\",\n",
    "    \"Reinforcement learning relies on an agent interacting with an environment, learning through trial and error to maximize cumulative rewards, and has shown great potential in areas like robotics and game playing.\",\n",
    "    \"One of the challenges in modern machine learning is overfitting, where a model performs well on training data but fails to generalize to new, unseen data due to excessive complexity or noise in the training set.\",\n",
    "    \"Regularization techniques, such as L1 and L2 regularization, are commonly used to mitigate overfitting by adding penalty terms to the model's loss function, discouraging overly complex solutions.\",\n",
    "    \"Feature engineering plays a crucial role in machine learning, involving the selection, extraction, and transformation of relevant features from raw data to enhance a model's predictive performance.\",\n",
    "    \"With the advent of deep learning, feature learning has become more automated, allowing neural networks to automatically learn hierarchical representations from raw data, reducing the need for manual feature engineering.\",\n",
    "    \"Convolutional Neural Networks (CNNs) have revolutionized image recognition tasks, utilizing convolutional layers to capture local patterns and hierarchical structures in images, achieving state-of-the-art performance.\",\n",
    "    \"Recurrent Neural Networks (RNNs) are widely used in natural language processing tasks, capable of capturing sequential dependencies and long-term contextual information, making them effective for tasks like language translation and sentiment analysis.\",\n",
    "    \"Generative Adversarial Networks (GANs) have gained attention for their ability to generate realistic synthetic data by training a generator network to compete against a discriminator network, resulting in a creative and data-driven approach.\",\n",
    "    \"Machine learning models are typically evaluated using metrics such as accuracy, precision, recall, F1 score, and area under the curve (AUC), providing quantitative measures of their performance on specific tasks.\",\n",
    "    \"Cross-validation is a commonly used technique to assess a model's generalization ability by splitting the data into multiple subsets for training and evaluation, helping to estimate its performance on unseen data.\",\n",
    "    \"The bias-variance tradeoff is a fundamental concept in machine learning, balancing the model's ability to fit the training data well (low bias) while avoiding overfitting (low variance) to improve generalization.\",\n",
    "    \"Ensemble learning methods, such as random forests and boosting, combine multiple base models to improve predictive performance and reduce the risk of overfitting, resulting in more robust and accurate models.\",\n",
    "    \"Hyperparameter tuning involves optimizing the settings or configurations of a machine learning model, such as learning rate, regularization strength, and network architecture, to achieve better performance.\",\n",
    "    \"Grid search and random search are common approaches for hyperparameter tuning, systematically exploring the hyperparameter space to find the optimal combination that maximizes the model's performance.\",\n",
    "    \"Model selection is a critical step in machine learning, where different algorithms or models are compared and evaluated to identify the most suitable one for a specific task based on their performance and complexity.\",\n",
    "    \"The No Free Lunch (NFL) theorem states that no machine learning algorithm is universally superior to all others across all possible problems, emphasizing the importance of selecting the right algorithm for a given task.\",\n",
    "    \"The curse of dimensionality refers to the challenges that arise when working with high-dimensional data, as the data becomes sparser, and the risk of overfitting increases, requiring careful feature selection and dimensionality reduction techniques.\",\n",
    "    \"Deep learning models often require large amounts of labeled data for training, which can be a bottleneck in domains where labeled data is scarce or expensive to acquire, leading to the exploration of semi-supervised and unsupervised learning methods.\",\n",
    "    \"The availability of open-source machine learning frameworks and libraries, such as TensorFlow, PyTorch, and scikit-learn, has significantly contributed to the widespread adoption and accessibility of modern machine learning techniques.\",\n",
    "    \"The field of Explainable AI (XAI) aims to address the black-box nature of some machine learning models by providing interpretable explanations and insights into how the models make their predictions or decisions.\",\n",
    "    \"Interpretability techniques, such as feature importance analysis, saliency maps, and attention mechanisms, help users understand the underlying reasoning and factors that influence the model's outputs.\",\n",
    "    \"Ethical considerations in machine learning have gained attention, focusing on issues like bias and fairness, privacy, accountability, and transparency to ensure responsible and trustworthy deployment of AI systems.\",\n",
    "    \"Bias in machine learning can arise from biased training data, leading to discriminatory outcomes and reinforcing existing societal biases, highlighting the need for diverse and representative training datasets.\",\n",
    "    \"Fairness-aware machine learning algorithms aim to mitigate bias and ensure fair treatment across different demographic groups, promoting fairness and equality in decision-making processes.\",\n",
    "    \"Privacy concerns arise when dealing with sensitive data in machine learning, leading to the development of privacy-preserving techniques, such as differential privacy and federated learning, to protect individuals' data while still enabling model training.\",\n",
    "    \"Adversarial attacks pose a threat to machine learning models, where malicious actors manipulate input data to deceive the model and cause incorrect predictions, driving the development of adversarial defense mechanisms.\",\n",
    "    \"The field of reinforcement learning has seen remarkable advancements, with algorithms like Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO) achieving human-level performance in challenging tasks like playing complex games.\",\n",
    "    \"Robotics has benefited from machine learning techniques, enabling robots to learn from data and adapt their behaviors to interact with and navigate the physical world, leading to advancements in autonomous vehicles and robotic automation.\",\n",
    "    \"Machine learning has found applications in healthcare, including disease diagnosis, personalized treatment recommendation, drug discovery, and medical imaging analysis, aiding clinicians and improving patient outcomes.\",\n",
    "    \"In finance, machine learning models are used for tasks like fraud detection, credit scoring, algorithmic trading, and risk assessment, leveraging vast amounts of financial data to make accurate predictions and inform decision-making.\",\n",
    "    \"Natural language processing (NLP) has made significant strides, with models like BERT and GPT achieving state-of-the-art performance in tasks such as sentiment analysis, language translation, and question answering.\",\n",
    "    \"Machine learning is playing a crucial role in environmental sciences, helping analyze climate data, predict natural disasters, monitor wildlife, and support sustainability efforts through applications like precision agriculture.\",\n",
    "    \"The interpretability of machine learning models has become a regulatory requirement in some domains, such as healthcare, where explainable and transparent AI systems are necessary to ensure patient safety and regulatory compliance.\",\n",
    "    \"The field of machine learning continues to evolve rapidly, with ongoing research in areas like meta-learning, few-shot learning, lifelong learning, and continual learning, aiming to improve the capabilities and flexibility of AI systems.\",\n",
    "    \"As machine learning models become more complex and powerful, there is an increasing need for ethical guidelines, regulations, and frameworks to govern their development, deployment, and impact on society.\",\n",
    "    \"The responsible use of machine learning requires interdisciplinary collaboration, involving not only computer scientists and engineers but also experts in fields like ethics, law, sociology, and psychology.\",\n",
    "    \"Machine learning has the potential to transform industries and societies, driving innovation, improving efficiency, and addressing complex challenges, but it also requires careful consideration of its limitations and societal impact.\",\n",
    "    \"As advancements in hardware, algorithms, and data availability continue, the future of machine learning holds great promise, with the potential to tackle increasingly complex problems and unlock new possibilities across various domains.\",\n",
    "    \"Modern machine learning is an ever-evolving field, continuously pushing the boundaries of what machines can learn and achieve, and its impact on society will continue to grow in the years to come.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalableSemanticSearch:\n",
    "    \"\"\"Vector similarity using product quantization with sentence transformers embeddings and cosine similarity.\"\"\"\n",
    "\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.model = SentenceTransformer(\n",
    "            \"sentence-transformers/all-mpnet-base-v2\", device=self.device\n",
    "        )\n",
    "        # self.model = SentenceTransformer('bert-base-nli-mean-tokens',  device=self.device)\n",
    "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
    "        self.quantizer = None\n",
    "        self.index = None\n",
    "        self.hashmap_index_sentence = None\n",
    "\n",
    "        log_directory = \"log\"\n",
    "        if not os.path.exists(log_directory):\n",
    "            os.makedirs(log_directory)\n",
    "        log_file_path = os.path.join(log_directory, \"scalable_semantic_search.log\")\n",
    "\n",
    "        logging.basicConfig(\n",
    "            filename=log_file_path,\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "        )\n",
    "        logging.info(\"ScalableSemanticSearch initialized with device: %s\", self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_clusters(n_data_points: int) -> int:\n",
    "        return max(2, min(n_data_points, int(np.sqrt(n_data_points))))\n",
    "\n",
    "    def encode(self, data: List[str]) -> np.ndarray:\n",
    "        \"\"\"Encode input data using sentence transformer model.\n",
    "\n",
    "        Args:\n",
    "            data: List of input sentences.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array of encoded sentences.\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(data)\n",
    "        self.hashmap_index_sentence = self.index_to_sentence_map(data)\n",
    "        return embeddings.astype(\"float32\")\n",
    "\n",
    "    def build_index(self, embeddings: np.ndarray) -> None:\n",
    "        \"\"\"Build the index for FAISS search.\n",
    "\n",
    "        Args:\n",
    "            embeddings: Numpy array of encoded sentences.\n",
    "        \"\"\"\n",
    "        n_data_points = len(embeddings)\n",
    "        if (\n",
    "            n_data_points >= 500\n",
    "        ):  # Adjust this value based on the minimum number of data points required for IndexIVFPQ\n",
    "            self.quantizer = faiss.IndexFlatL2(self.dimension)\n",
    "            n_clusters = self.calculate_clusters(n_data_points)\n",
    "            self.index = faiss.IndexIVFPQ(\n",
    "                self.quantizer, self.dimension, n_clusters, 8, 4\n",
    "            )\n",
    "            logging.info(\"IndexIVFPQ created with %d clusters\", n_clusters)\n",
    "        else:\n",
    "            self.index = faiss.IndexFlatL2(self.dimension)\n",
    "            logging.info(\"IndexFlatL2 created\")\n",
    "\n",
    "        if isinstance(self.index, faiss.IndexIVFPQ):\n",
    "            self.index.train(embeddings)\n",
    "        self.index.add(embeddings)\n",
    "        logging.info(\"Index built on device: %s\", self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def index_to_sentence_map(data: List[str]) -> Dict[int, str]:\n",
    "        \"\"\"Create a mapping between index and sentence.\n",
    "\n",
    "        Args:\n",
    "            data: List of sentences.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping index to the corresponding sentence.\n",
    "        \"\"\"\n",
    "        return {index: sentence for index, sentence in enumerate(data)}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_top_sentences(\n",
    "        index_map: Dict[int, str], top_indices: np.ndarray\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Get the top sentences based on the indices.\n",
    "\n",
    "        Args:\n",
    "            index_map: Dictionary mapping index to the corresponding sentence.\n",
    "            top_indices: Numpy array of top indices.\n",
    "\n",
    "        Returns:\n",
    "            List of top sentences.\n",
    "        \"\"\"\n",
    "        return [index_map[i] for i in top_indices]\n",
    "\n",
    "    def search(self, input_sentence: str, top: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Compute cosine similarity between an input sentence and a collection of sentence embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_sentence: The input sentence to compute similarity against.\n",
    "            top: The number of results to return.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing two numpy arrays. The first array contains the cosine similarities between the input\n",
    "            sentence and the embeddings, ordered in descending order. The second array contains the indices of the\n",
    "            corresponding embeddings in the original array, also ordered by descending similarity.\n",
    "        \"\"\"\n",
    "        vectorized_input = self.model.encode(\n",
    "            [input_sentence], device=self.device\n",
    "        ).astype(\"float32\")\n",
    "        D, I = self.index.search(vectorized_input, top)\n",
    "        return I[0], 1 - D[0]\n",
    "\n",
    "    def save_index(self, file_path: str) -> None:\n",
    "        \"\"\"Save the FAISS index to disk.\n",
    "\n",
    "        Args:\n",
    "            file_path: The path where the index will be saved.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"index\"):\n",
    "            faiss.write_index(self.index, file_path)\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"The index has not been built yet. Build the index using `build_index` method first.\"\n",
    "            )\n",
    "\n",
    "    def load_index(self, file_path: str) -> None:\n",
    "        \"\"\"Load a previously saved FAISS index from disk.\n",
    "\n",
    "        Args:\n",
    "            file_path: The path where the index is stored.\n",
    "        \"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            self.index = faiss.read_index(file_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"The specified file '{file_path}' does not exist.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def measure_time(func: Callable, *args, **kwargs) -> Tuple[float, Any]:\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        return elapsed_time, result\n",
    "\n",
    "    @staticmethod\n",
    "    def measure_memory_usage() -> float:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        ram = process.memory_info().rss\n",
    "        return ram / (1024**2)\n",
    "\n",
    "    def timed_train(self, data: List[str]) -> Tuple[float, float]:\n",
    "        start_time = time.time()\n",
    "        embeddings = self.encode(data)\n",
    "        self.build_index(embeddings)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        memory_usage = self.measure_memory_usage()\n",
    "        logging.info(\n",
    "            \"Training time: %.2f seconds on device: %s\", elapsed_time, self.device\n",
    "        )\n",
    "        logging.info(\"Training memory usage: %.2f MB\", memory_usage)\n",
    "        return elapsed_time, memory_usage\n",
    "\n",
    "    def timed_infer(self, query: str, top: int) -> Tuple[float, float]:\n",
    "        start_time = time.time()\n",
    "        _, _ = self.search(query, top)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        memory_usage = self.measure_memory_usage()\n",
    "        logging.info(\n",
    "            \"Inference time: %.2f seconds on device: %s\", elapsed_time, self.device\n",
    "        )\n",
    "        logging.info(\"Inference memory usage: %.2f MB\", memory_usage)\n",
    "        return elapsed_time, memory_usage\n",
    "\n",
    "    def timed_load_index(self, file_path: str) -> float:\n",
    "        start_time = time.time()\n",
    "        self.load_index(file_path)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        logging.info(\n",
    "            \"Index loading time: %.2f seconds on device: %s\", elapsed_time, self.device\n",
    "        )\n",
    "        return elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception when trying to download https://sbert.net/models/sentence-transformers/all-mpnet-base-v2.zip. Response 404\n"
     ]
    }
   ],
   "source": [
    "semantic_search = ScalableSemanticSearch(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = semantic_search.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 768)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_search.build_index(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explainable AI (Artificial Intelligence) is becoming increasingly important in modern machi\"\n",
    "top = 3\n",
    "top_indices, top_scores = semantic_search.search(query, top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 32, 45])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sentences = ScalableSemanticSearch.get_top_sentences(semantic_search.hashmap_index_sentence, top_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Explainable AI (Artificial Intelligence) is becoming increasingly important in modern machine learning, as it aims to provide understandable and interpretable explanations for the decisions made by AI systems.',\n",
       " 'Modern machine learning involves the use of sophisticated algorithms and computational models to enable computers to learn from data and make predictions or decisions without being explicitly programmed.',\n",
       " 'Deep learning, a subset of machine learning, has gained prominence in recent years, utilizing neural networks with multiple layers to process complex data and extract meaningful patterns.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kflm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

<!-- PROJECT LOGO -->
<br />


<!-- RESEASE NOTES -->
## Release Notes
Release 0.1: This repository contains the Test and evaluation framework for AI reasoning. It includes prompt-based evaluation framework in a [Jupyter notebook for AI reasoning](#evaluation.ipynb) 

## Introduction
This codebase implements a prompt-based evaluation framework to measure the effectiveness of the AI-powered reasoning engine. We demonstrate the utility of the framework with [SciRepEval](https://github.com/allenai/scirepeval) benchmark for scientific document reasoning. We plan to include novel in-domain benchmarks in the future.
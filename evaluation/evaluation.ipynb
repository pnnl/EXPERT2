{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Evaluation Framework for AI Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the config file available in this package. This file contains different parameter settings specific to each task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "#load the config details- \n",
    "with open('configs/config.json', 'r') as openfile:\n",
    "    config = json.load(openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the input and output directories. Input the task type. (Allowed task types - \"biomimicry\", \"DRSM\", \"Field of Study\", \"MeSH Descriptors\", \"SciDocs-Mesh Diseases\", \"SciDocs-MAG\", \"Nuclear\")<br>\n",
    "If the task type is DRSM, provide two output paths in array format (each for gold label and standard label types)<br>\n",
    "If the task type is SciDocs-Mesh Diseases or SciDocs-MAG, provide two input paths in array format (first path containing the data and second path containing the label and doc id mappings). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instructions import instructions\n",
    "import os\n",
    "\n",
    "input_dir = \"./demo/inputs/sample.jsonl\"\n",
    "\n",
    "out_dir = \"./demo/inputs/instructions_sample_train.jsonl\"\n",
    "\n",
    "task = \"Field of Study\"\n",
    "\n",
    "\n",
    "# Initialize class\n",
    "iGen = instructions.InstructionsGenerator(input_dir, out_dir, task, config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Physics',\n",
       " 'Biology',\n",
       " 'Geography',\n",
       " 'Political science',\n",
       " 'Business',\n",
       " 'Psychology',\n",
       " 'Materials science',\n",
       " 'Engineering',\n",
       " 'Linguistics',\n",
       " 'Geology',\n",
       " 'Art',\n",
       " 'Economics',\n",
       " 'Mathematics',\n",
       " 'Agricultural and Food sciences',\n",
       " 'Philosophy',\n",
       " 'History',\n",
       " 'Medicine',\n",
       " 'Sociology',\n",
       " 'Law',\n",
       " 'Computer science',\n",
       " 'Chemistry',\n",
       " 'Education',\n",
       " 'Environmental science']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all possible categories \n",
    "all_cat = iGen.get_all_categories()\n",
    "all_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate instruction sample\n",
    "ins_sample = iGen.generate_instructions(all_cat)\n",
    "ins_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File(s) saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Save the result to file\n",
    "iGen.save_to_file(ins_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune import finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the values of all the parameters for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFAULT PARAMETERS\n",
    "train_retriever = True # Recommended True. whether to train the retriever or not.\n",
    "query_side_retriever_training=True # Recommended True. only trains the query encoder whereas passage encoder remains frozen.\n",
    "reader_model_type= \"google/t5-base-lm-adapt\" # name of the reader model\n",
    "model_path= \"../checkpoint/step-4800/\" # path of the pretrained model checkpoint\n",
    "train_data = [\"./demo/inputs/instructions_sample_train.jsonl\"] # list of paths to train files\n",
    "eval_data = [\"./demo/inputs/instructions_sample_test.jsonl\"] # list of paths to evaluation files\n",
    "name = \"atlas-mlm-gen-S2ROC-220M-fos-instun\" # name of the experiment or directory where instrution tuning checkpoint would be saved\n",
    "checkpoint_dir = \"./experiments\" # source Directory where experiment directory is created\n",
    "train_steps = 10000 # number of finetuning steps\n",
    "load_index_path = \"./saved_index\" # path to passage index and embeddings, please reference to the script `model/custom_train_genindex.py`.\n",
    "load_subset_textindex = True # Recoomended True for low compute scenarios. Load subset of index rather than entire bulk.\n",
    "subset_textindex_name = \"Physics,Bio-1,Art,History,Political-Science,Business,Economics,Geology,Computer-Science,Environmental-Science,Engineering,Med-1\" # name of the domain indexes that need to be load for retrieval\n",
    "task = \"base\" # task type. For classification use \"base\". For open domain QA, use \"QA\".\n",
    "gold_score_mode = \"pdist\" # target scores type for loss evaluation.\n",
    "per_gpu_batch_size = 2 # number of queries per GPU. Global bacth size = per_gpu_batch_size* number of GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the following to True if you want the model to use the arguments you gave earlier. Otherwise the model will default to default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUSTOM_ARGS = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the instruction finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune.InstructionFinetuning.run_instructions_finetune(train_retriever,\n",
    "                query_side_retriever_training,\n",
    "                reader_model_type,\n",
    "                model_path,\n",
    "                train_data,\n",
    "                eval_data, \n",
    "                name,\n",
    "                checkpoint_dir,\n",
    "                train_steps,\n",
    "                load_index_path,\n",
    "                load_subset_textindex,\n",
    "                subset_textindex_name,\n",
    "                task,\n",
    "                gold_score_mode,\n",
    "                per_gpu_batch_size,\n",
    "                USE_CUSTOM_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune import finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the values of all the parameters for the Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFAULT PARAMETERS\n",
    "reader_model_type= \"google/t5-base-lm-adapt\" # name of the reader model\n",
    "model_path= \"\" #  path of the pretrained model checkpoint, please use the model checkpoint available in BDC.\n",
    "eval_data = [\"./demo/inputs/instructions_sample_test.jsonl\"] # list of paths to eval files\n",
    "name = \"atlas-mlm-gen-S2ROC-220M-fosinstun-foseval-step9690-adapretrv2\" # name of the experiment or directory where instrution tuning checkpoint would be saved\n",
    "checkpoint_dir = \"./experiments\" # source Directory where experiment directory is created\n",
    "load_index_path = \"./saved_index\" # path to passage index and embeddings\n",
    "load_subset_textindex = True  # Recoomended True for low compute scenarios. Load subset of index rather than entire bulk.\n",
    "task = \"base\" # task type. For classification use \"base\". For open domain QA, use \"QA\".\n",
    "gold_score_mode = \"pdist\" # target scores type for loss evaluation.\n",
    "per_gpu_batch_size = 2 # number of queries per GPU. Global bacth size = per_gpu_batch_size* number of GPUs.\n",
    "per_gpu_batch_size_domainindex= 600 # number of queries per GPU for domain searching.\n",
    "no_sel_indices= 3 # number of domains for each type of query\n",
    "index_model_path = \"./base/\" # path of the domain search model checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the following to True if you want the model to use the arguments you gave earlier. Otherwise the model will default to default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUSTOM_ARGS = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune.Evaluation.run_generation(reader_model_type,\n",
    "                    model_path,\n",
    "                    eval_data, \n",
    "                    name,\n",
    "                    checkpoint_dir,\n",
    "                    load_index_path,\n",
    "                    load_subset_textindex,\n",
    "                    task,\n",
    "                    gold_score_mode,\n",
    "                    per_gpu_batch_size,\n",
    "                    per_gpu_batch_size_domainindex,\n",
    "                    no_sel_indices,\n",
    "                    index_model_path,\n",
    "                    USE_CUSTOM_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune.Evaluation.run_metrics(\n",
    "                    filepath=\"./demo/inputs/instructions_sample_test_outputs.jsonl\",\n",
    "                    outpath=\"./demo/inputs/\",\n",
    "                    metric_name=\"accuracy\",\n",
    "                    task_type=\"fos\",\n",
    "                    paper_topic_file=\"./demo/inputs/paper_topic_map.json\", ## Please provide a file that contains the mapping between document to scientific domains.\n",
    "                    USE_CUSTOM_ARGS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy = pd.read_json(\"./demo/inputs/instructions_sample_test_outputs_metrics_accuracy.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evidence Generation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune.Evaluation.run_metrics(\n",
    "                    filepath=\"./demo/inputs/instructions_sample_test_outputs.jsonl\",\n",
    "                    outpath=\"./demo/inputs\",\n",
    "                    metric_name=\"topic\",\n",
    "                    task_type=\"fos\",\n",
    "                    paper_topic_file=\"/home/evaluation/paper_topic_map.json\", ## Please provide a file that contains the mapping between document to scientific domains.\n",
    "                    USE_CUSTOM_ARGS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_relevance = pd.read_json(\"./demo/inputs/instructions_sample_test_outputs_metrics_evidence.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
